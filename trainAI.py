import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque
import random
import matplotlib.pyplot as plt
import os

# RLCard ì„¤ì¹˜: pip install rlcard
try:
    import rlcard
    from rlcard.agents import RandomAgent
    from rlcard.envs.registration import make
    print("âœ… RLCard ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print("âŒ RLCard ì„¤ì¹˜ í•„ìš”: pip install rlcard")
    raise e

class SimpleDQN(nn.Module):
    def __init__(self, input_dim, hidden_dim=256, output_dim=4):
        super(SimpleDQN, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, hidden_dim//2)
        self.fc4 = nn.Linear(hidden_dim//2, output_dim)
        self.dropout = nn.Dropout(0.1)
    
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = F.relu(self.fc3(x))
        x = self.fc4(x)
        return x

class CustomDQNAgent:
    def __init__(self, input_dim, action_num, lr=0.001, device=None):
        # input_dimì„ ì§ì ‘ ë°›ì•„ì„œ ì •ìˆ˜ë¡œ ë³´ì¥
        self.input_dim = int(input_dim)
        self.action_num = action_num
        self.lr = lr
        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print(f"ì—ì´ì „íŠ¸ ì´ˆê¸°í™” - ì…ë ¥ ì°¨ì›: {self.input_dim}, ì•¡ì…˜ ìˆ˜: {action_num}")
        
        # ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        self.q_net = SimpleDQN(self.input_dim, output_dim=action_num).to(self.device)
        self.target_net = SimpleDQN(self.input_dim, output_dim=action_num).to(self.device)
        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„°
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.gamma = 0.95
        self.batch_size = 32
        self.memory_size = 10000
        
        # ê²½í—˜ ë¦¬í”Œë ˆì´ ë²„í¼
        self.memory = deque(maxlen=self.memory_size)
        
        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        self.target_net.load_state_dict(self.q_net.state_dict())
        
        self.train_step = 0
        self.update_target_every = 100
    
    def feed(self, ts):
        """RLCard ì¸í„°í˜ì´ìŠ¤ì— ë§ì¶˜ ê²½í—˜ ì €ì¥"""
        try:
            if len(ts) >= 4:
                if len(ts) == 4:
                    state, action, reward, next_state = ts
                    done = False
                else:
                    state, action, reward, next_state, done = ts[:5]
                
                self.memory.append((state, action, reward, next_state, done))
        except Exception as e:
            print(f"feed ì˜¤ë¥˜: {e}")
    
    def step(self, state):
        """ì•¡ì…˜ ì„ íƒ - RLCard í˜¸í™˜ (í›ˆë ¨ìš©)"""
        return self._select_action(state, use_epsilon=True)
    
    def eval_step(self, state):
        """í‰ê°€ìš© ì•¡ì…˜ ì„ íƒ - RLCard í˜¸í™˜ (í‰ê°€ìš©, íƒí—˜ ì—†ìŒ)"""
        return self._select_action(state, use_epsilon=False)
    
    def _select_action(self, state, use_epsilon=True):
        """ë‚´ë¶€ ì•¡ì…˜ ì„ íƒ ë¡œì§"""
        try:
            # RLCard í™˜ê²½ì—ì„œ stateëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœ
            if isinstance(state, dict):
                obs = state['obs']
                legal_actions = state['legal_actions']
            else:
                obs = state
                legal_actions = list(range(self.action_num))
            
            # ìƒíƒœ ê²€ì¦
            if obs is None:
                return int(np.random.choice(legal_actions))
            
            # íƒí—˜ vs í™œìš© (í‰ê°€ ì‹œì—ëŠ” íƒí—˜ ì•ˆí•¨)
            if use_epsilon and np.random.random() <= self.epsilon:
                return int(np.random.choice(legal_actions))
            
            # ìƒíƒœë¥¼ í…ì„œë¡œ ë³€í™˜ - ê³ ì • ì°¨ì›ìœ¼ë¡œ
            try:
                if isinstance(obs, (list, np.ndarray)):
                    obs_array = np.array(obs, dtype=np.float32).flatten()
                    # ê³ ì • ì°¨ì›ìœ¼ë¡œ ë§ì¶¤
                    if len(obs_array) >= self.input_dim:
                        obs_array = obs_array[:self.input_dim]
                    else:
                        padded_obs = np.zeros(self.input_dim, dtype=np.float32)
                        padded_obs[:len(obs_array)] = obs_array
                        obs_array = padded_obs
                    obs_tensor = torch.FloatTensor(obs_array).unsqueeze(0).to(self.device)
                else:
                    obs_array = np.zeros(self.input_dim, dtype=np.float32)
                    obs_array[0] = float(obs)
                    obs_tensor = torch.FloatTensor(obs_array).unsqueeze(0).to(self.device)
            except Exception as e:
                print(f"ìƒíƒœ ë³€í™˜ ì˜¤ë¥˜: {e}, obs: {obs}")
                return int(np.random.choice(legal_actions))
            
            # Qê°’ ê³„ì‚°
            with torch.no_grad():
                q_values = self.q_net(obs_tensor)
            
            # ìœ íš¨í•œ ì•¡ì…˜ë§Œ ê³ ë ¤
            if len(legal_actions) < self.action_num:
                masked_q_values = q_values.clone()
                for i in range(self.action_num):
                    if i not in legal_actions:
                        masked_q_values[0][i] = -float('inf')
                q_values = masked_q_values
            
            action = q_values.cpu().data.numpy().argmax()
            
            # numpy.int64ë¥¼ Python intë¡œ ë³€í™˜
            action = int(action)
            
            # ìœ íš¨í•œ ì•¡ì…˜ì¸ì§€ í™•ì¸
            if action not in legal_actions:
                action = int(np.random.choice(legal_actions))
            
            return action
            
        except Exception as e:
            print(f"ì•¡ì…˜ ì„ íƒ ì˜¤ë¥˜: {e}")
            return int(np.random.choice(legal_actions)) if 'legal_actions' in locals() else 0
    
    def train(self):
        """ë„¤íŠ¸ì›Œí¬ í›ˆë ¨"""
        if len(self.memory) < self.batch_size:
            return 0
            
        try:
            # ë°°ì¹˜ ìƒ˜í”Œë§
            batch = random.sample(self.memory, self.batch_size)
            
            states = []
            actions = []
            rewards = []
            next_states = []
            dones = []
            
            for transition in batch:
                try:
                    state, action, reward, next_state, done = transition
                    
                    # ìƒíƒœ ì²˜ë¦¬
                    if isinstance(state, dict):
                        state_obs = state['obs']
                    else:
                        state_obs = state
                        
                    if isinstance(next_state, dict):
                        next_state_obs = next_state['obs']
                    else:
                        next_state_obs = next_state
                    
                    # None ì²´í¬
                    if state_obs is None or next_state_obs is None:
                        continue
                    
                    # ë°°ì—´ ë³€í™˜ - ê³ ì • ì°¨ì›ìœ¼ë¡œ
                    if isinstance(state_obs, (list, np.ndarray)):
                        state_obs = np.array(state_obs, dtype=np.float32).flatten()
                        if len(state_obs) >= self.input_dim:
                            state_obs = state_obs[:self.input_dim]
                        else:
                            padded_state = np.zeros(self.input_dim, dtype=np.float32)
                            padded_state[:len(state_obs)] = state_obs
                            state_obs = padded_state
                    else:
                        temp_obs = np.zeros(self.input_dim, dtype=np.float32)
                        temp_obs[0] = float(state_obs)
                        state_obs = temp_obs
                        
                    if isinstance(next_state_obs, (list, np.ndarray)):
                        next_state_obs = np.array(next_state_obs, dtype=np.float32).flatten()
                        if len(next_state_obs) >= self.input_dim:
                            next_state_obs = next_state_obs[:self.input_dim]
                        else:
                            padded_next_state = np.zeros(self.input_dim, dtype=np.float32)
                            padded_next_state[:len(next_state_obs)] = next_state_obs
                            next_state_obs = padded_next_state
                    else:
                        temp_next_obs = np.zeros(self.input_dim, dtype=np.float32)
                        temp_next_obs[0] = float(next_state_obs)
                        next_state_obs = temp_next_obs
                    
                    states.append(state_obs)
                    actions.append(int(action))
                    rewards.append(float(reward))
                    next_states.append(next_state_obs)
                    dones.append(bool(done))
                    
                except Exception as e:
                    print(f"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
            
            if len(states) == 0:
                return 0
            
            # í…ì„œ ë³€í™˜
            try:
                states = torch.FloatTensor(states).to(self.device)
                actions = torch.LongTensor(actions).to(self.device)
                rewards = torch.FloatTensor(rewards).to(self.device)
                next_states = torch.FloatTensor(next_states).to(self.device)
                dones = torch.BoolTensor(dones).to(self.device)
            except Exception as e:
                print(f"í…ì„œ ë³€í™˜ ì˜¤ë¥˜: {e}")
                print(f"states í˜•íƒœ í™•ì¸: {[s.shape if hasattr(s, 'shape') else type(s) for s in states[:3]]}")
                return 0
            
            # Q ê°’ ê³„ì‚°
            current_q_values = self.q_net(states).gather(1, actions.unsqueeze(1))
            next_q_values = self.target_net(next_states).max(1)[0].detach()
            target_q_values = rewards + (self.gamma * next_q_values * ~dones)
            
            # ì†ì‹¤ ê³„ì‚° ë° ì—­ì „íŒŒ
            loss = F.mse_loss(current_q_values.squeeze(), target_q_values)
            
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), 1.0)
            self.optimizer.step()
            
            # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
            self.train_step += 1
            if self.train_step % self.update_target_every == 0:
                self.target_net.load_state_dict(self.q_net.state_dict())
            
            # ì…ì‹¤ë¡  ê°ì†Œ
            if self.epsilon > self.epsilon_min:
                self.epsilon *= self.epsilon_decay
            
            return loss.item()
            
        except Exception as e:
            print(f"í›ˆë ¨ ì¤‘ ì˜¤ë¥˜: {e}")
            return 0
    
    def save_model(self, path):
        """ëª¨ë¸ ì €ì¥ - Poker.pyì™€ í˜¸í™˜ë˜ë„ë¡ ìˆ˜ì •"""
        try:
            # ê²½ë¡œ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(os.path.dirname(path) if os.path.dirname(path) else '.', exist_ok=True)
            
            # Poker.pyì—ì„œ ì½ì„ ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ì €ì¥
            torch.save({
                'q_net_state_dict': self.q_net.state_dict(),
                'target_net_state_dict': self.target_net.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'epsilon': self.epsilon,
                'train_step': self.train_step,
                'input_dim': self.input_dim,
                'action_num': self.action_num,
                # Poker.pyì—ì„œ ì‚¬ìš©í•˜ëŠ” í‚¤ ì¶”ê°€
                'model_state_dict': self.q_net.state_dict(),
                'model_config': {
                    'input_dim': self.input_dim,
                    'hidden_dim': 256,
                    'output_dim': self.action_num
                }
            }, path)
            print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {path}")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def load_model(self, path):
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            checkpoint = torch.load(path, map_location=self.device)
            self.q_net.load_state_dict(checkpoint['q_net_state_dict'])
            self.target_net.load_state_dict(checkpoint['target_net_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.epsilon = checkpoint['epsilon']
            self.train_step = checkpoint['train_step']
            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {path}")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

def normalize_observation(obs_data, target_dim):
    """ê´€ì°° ë°ì´í„°ë¥¼ target_dim í¬ê¸°ë¡œ ì •ê·œí™”"""
    try:
        if obs_data is None:
            return np.zeros(target_dim, dtype=np.float32)
        
        # numpy ë°°ì—´ë¡œ ë³€í™˜
        if isinstance(obs_data, (list, tuple)):
            obs_array = np.array(obs_data, dtype=np.float32)
        elif isinstance(obs_data, np.ndarray):
            obs_array = obs_data.astype(np.float32)
        elif isinstance(obs_data, (int, float)):
            obs_array = np.array([float(obs_data)], dtype=np.float32)
        else:
            return np.zeros(target_dim, dtype=np.float32)
        
        # 1ì°¨ì›ìœ¼ë¡œ í‰íƒ„í™”
        obs_array = obs_array.flatten()
        
        # í¬ê¸° ì¡°ì •
        if len(obs_array) >= target_dim:
            return obs_array[:target_dim]
        else:
            # 0ìœ¼ë¡œ íŒ¨ë”©
            result = np.zeros(target_dim, dtype=np.float32)
            result[:len(obs_array)] = obs_array
            return result
            
    except Exception as e:
        print(f"ê´€ì°° ì •ê·œí™” ì˜¤ë¥˜: {e}")
        return np.zeros(target_dim, dtype=np.float32)

def train_poker_agent(num_episodes=1000):
    """í¬ì»¤ ì—ì´ì „íŠ¸ í›ˆë ¨"""
    print("ğŸ° í…ì‚¬ìŠ¤ í™€ë¤ ì—ì´ì „íŠ¸ í›ˆë ¨ ì‹œì‘!")
    
    try:
        # RLCard í™˜ê²½ ìƒì„±
        env = make('no-limit-holdem', config={'seed': 42})
        print(f"âœ… í™˜ê²½ ìƒì„± ì™„ë£Œ: {env.game.get_num_players()}ëª… í”Œë ˆì´ì–´")
        
        # í™˜ê²½ ì •ë³´ ì¶œë ¥ ë° ìƒíƒœ í¬ê¸° í™•ì¸
        print(f"ì›ë³¸ ìƒíƒœ í¬ê¸°: {env.state_shape}")
        print(f"ì•¡ì…˜ ìˆ˜: {env.num_actions}")
        print(f"í”Œë ˆì´ì–´ ìˆ˜: {env.num_players}")
        
        # ì‹¤ì œ ìƒíƒœ ì°¨ì› í™•ì¸ì„ ìœ„í•œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        print("ğŸ” ì‹¤ì œ ìƒíƒœ ì°¨ì› í™•ì¸ ì¤‘...")
        test_state, _ = env.reset()
        actual_obs = test_state['obs'] if isinstance(test_state, dict) else test_state
        
        if isinstance(actual_obs, (list, np.ndarray)):
            actual_obs = np.array(actual_obs).flatten()
            actual_input_dim = len(actual_obs)
        else:
            actual_input_dim = 1
            
        print(f"ì‹¤ì œ ê´€ì°° ì°¨ì›: {actual_input_dim}")
        print(f"í™˜ê²½ state_shape: {env.state_shape}")
        
        # ì‹¤ì œ ì°¨ì›ì„ ì‚¬ìš©
        input_dim = actual_input_dim
        print(f"ì‚¬ìš©í•  ì…ë ¥ ì°¨ì›: {input_dim}")
        
        # ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        agent = CustomDQNAgent(input_dim, env.num_actions)
        print(f"ë””ë°”ì´ìŠ¤: {agent.device}")
        
        # ë‹¤ë¥¸ í”Œë ˆì´ì–´ë“¤ì€ ëœë¤ ì—ì´ì „íŠ¸ë¡œ ì„¤ì •
        agents = [agent]
        for i in range(env.num_players - 1):
            agents.append(RandomAgent(env.num_actions))
        
        env.set_agents(agents)
        
        # í›ˆë ¨ ê¸°ë¡
        episode_rewards = []
        losses = []
        win_rates = []
        
        for episode in range(num_episodes):
            try:
                # ê²Œì„ ì‹¤í–‰
                trajectories, payoffs = env.run(is_training=True)
                
                # ê²½í—˜ ì €ì¥
                if trajectories and len(trajectories) > 0:
                    agent_trajectory = trajectories[0]
                    
                    for i, ts in enumerate(agent_trajectory):
                        is_done = (i == len(agent_trajectory) - 1)
                        
                        if len(ts) >= 4:
                            state, action, reward, next_state = ts[:4]
                            
                            # ìƒíƒœ ì²˜ë¦¬
                            if isinstance(state, dict) and 'obs' in state:
                                obs = normalize_observation(state['obs'], input_dim)
                                modified_state = {
                                    'obs': obs, 
                                    'legal_actions': state.get('legal_actions', list(range(env.num_actions)))
                                }
                            else:
                                obs = normalize_observation(state, input_dim)
                                modified_state = {
                                    'obs': obs, 
                                    'legal_actions': list(range(env.num_actions))
                                }
                            
                            # next_state ì²˜ë¦¬
                            if isinstance(next_state, dict) and 'obs' in next_state:
                                next_obs = normalize_observation(next_state['obs'], input_dim)
                                modified_next_state = {
                                    'obs': next_obs, 
                                    'legal_actions': next_state.get('legal_actions', list(range(env.num_actions)))
                                }
                            else:
                                next_obs = normalize_observation(next_state, input_dim)
                                modified_next_state = {
                                    'obs': next_obs, 
                                    'legal_actions': list(range(env.num_actions))
                                }
                            
                            agent.feed((modified_state, action, reward, modified_next_state, is_done))
                
                # ì—ì´ì „íŠ¸ í›ˆë ¨
                if len(agent.memory) >= agent.batch_size:
                    loss = agent.train()
                    if loss > 0:
                        losses.append(loss)
                
                # ë³´ìƒ ê¸°ë¡
                reward = payoffs[0] if payoffs else 0
                episode_rewards.append(reward)
                
                # ìŠ¹ë¥  ê³„ì‚°
                if len(episode_rewards) >= 100:
                    recent_rewards = episode_rewards[-100:]
                    wins = sum(1 for r in recent_rewards if r > 0)
                    win_rate = wins / len(recent_rewards)
                    win_rates.append(win_rate)
                
                # ì§„í–‰ìƒí™© ì¶œë ¥
                if episode % 100 == 0 and episode > 0:
                    avg_reward = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)
                    current_win_rate = win_rates[-1] if win_rates else 0
                    print(f"ì—í”¼ì†Œë“œ {episode:5d} | "
                          f"í‰ê·  ë³´ìƒ: {avg_reward:6.2f} | "
                          f"ìŠ¹ë¥ : {current_win_rate:5.1%} | "
                          f"ì…ì‹¤ë¡ : {agent.epsilon:.3f} | "
                          f"ë©”ëª¨ë¦¬: {len(agent.memory)}")
                
                # ëª¨ë¸ ì €ì¥
                if episode % 500 == 0 and episode > 0:
                    agent.save_model(f'poker_model_{episode}.pth')
                    
            except Exception as e:
                print(f"ì—í”¼ì†Œë“œ {episode}ì—ì„œ ì˜¤ë¥˜: {e}")
                continue
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        agent.save_model('final_poker_model.pth')
        
        return agent, episode_rewards, losses, win_rates
        
    except Exception as e:
        print(f"âŒ í›ˆë ¨ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        raise e

def evaluate_agent(agent, num_games=200):
    """í›ˆë ¨ëœ ì—ì´ì „íŠ¸ í‰ê°€"""
    print(f"\nğŸ¯ ì—ì´ì „íŠ¸ í‰ê°€ ({num_games}ê²Œì„)")
    
    try:
        # í‰ê°€ìš© í™˜ê²½
        env = make('no-limit-holdem', config={'seed': 123})
        
        # ë‹¤ë¥¸ í”Œë ˆì´ì–´ë“¤ì€ ëœë¤
        agents = [agent]
        for i in range(env.num_players - 1):
            agents.append(RandomAgent(env.num_actions))
        env.set_agents(agents)
        
        payoffs = []
        wins = 0
        
        for game in range(num_games):
            try:
                # í‰ê°€ ëª¨ë“œë¡œ ì‹¤í–‰ (is_training=False)
                _, game_payoffs = env.run(is_training=False)
                payoff = game_payoffs[0]
                payoffs.append(payoff)
                
                if payoff > 0:
                    wins += 1
                
                if game % 50 == 0:
                    print(f"ê²Œì„ {game:4d} ì™„ë£Œ...")
                    
            except Exception as e:
                print(f"ê²Œì„ {game}ì—ì„œ ì˜¤ë¥˜: {e}")
                continue
        
        # ê²°ê³¼ ë¶„ì„
        if payoffs:
            avg_payoff = np.mean(payoffs)
            win_rate = wins / len(payoffs)
            std_payoff = np.std(payoffs)
            
            print(f"\nğŸ“Š í‰ê°€ ê²°ê³¼:")
            print(f"ì™„ë£Œëœ ê²Œì„: {len(payoffs)}")
            print(f"ìŠ¹ë¦¬: {wins}")
            print(f"ìŠ¹ë¥ : {win_rate:.1%}")
            print(f"í‰ê·  ë³´ìƒ: {avg_payoff:.3f}")
            print(f"ë³´ìƒ í‘œì¤€í¸ì°¨: {std_payoff:.3f}")
        else:
            avg_payoff, win_rate = 0, 0
            print("âŒ í‰ê°€í•  ìˆ˜ ìˆëŠ” ê²Œì„ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        return avg_payoff, win_rate, payoffs
        
    except Exception as e:
        print(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
        return 0, 0, []

def plot_training_results(rewards, losses, win_rates):
    """í›ˆë ¨ ê²°ê³¼ ì‹œê°í™”"""
    try:
        plt.figure(figsize=(15, 10))
        
        # 2x2 ì„œë¸Œí”Œë¡¯
        plt.subplot(2, 2, 1)
        if rewards:
            plt.plot(rewards, alpha=0.3, color='blue', label='ì›ë³¸')
            if len(rewards) >= 50:
                smoothed = [np.mean(rewards[max(0, i-49):i+1]) for i in range(len(rewards))]
                plt.plot(smoothed, color='red', linewidth=2, label='í‰ê· (50)')
            plt.title('ì—í”¼ì†Œë“œë³„ ë³´ìƒ')
            plt.xlabel('ì—í”¼ì†Œë“œ')
            plt.ylabel('ë³´ìƒ')
            plt.legend()
            plt.grid(True)
        
        plt.subplot(2, 2, 2)
        if losses:
            plt.plot(losses, color='orange')
            plt.title('í›ˆë ¨ ì†ì‹¤')
            plt.xlabel('í›ˆë ¨ ìŠ¤í…')
            plt.ylabel('ì†ì‹¤')
            plt.grid(True)
        
        plt.subplot(2, 2, 3)
        if win_rates:
            plt.plot(win_rates, color='green', linewidth=2)
            plt.title('ìŠ¹ë¥  (100ê²Œì„ ë‹¨ìœ„)')
            plt.xlabel('ì—í”¼ì†Œë“œ (x100)')
            plt.ylabel('ìŠ¹ë¥ ')
            plt.ylim(0, 1)
            plt.grid(True)
        
        plt.subplot(2, 2, 4)
        if rewards:
            plt.hist(rewards, bins=30, alpha=0.7, color='purple')
            plt.title('ë³´ìƒ ë¶„í¬')
            plt.xlabel('ë³´ìƒ')
            plt.ylabel('ë¹ˆë„')
            plt.grid(True)
        
        plt.tight_layout()
        plt.show()
        
    except Exception as e:
        print(f"ì‹œê°í™” ì¤‘ ì˜¤ë¥˜: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ² RLCardë¥¼ ì‚¬ìš©í•œ í…ì‚¬ìŠ¤ í™€ë¤ ê°•í™”í•™ìŠµ")
    print("=" * 50)
    
    try:
        # í›ˆë ¨ (í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì ì€ ì—í”¼ì†Œë“œ)
        print("1ï¸âƒ£ ì—ì´ì „íŠ¸ í›ˆë ¨ ì¤‘...")
        agent, rewards, losses, win_rates = train_poker_agent(num_episodes=500)
        
        # í‰ê°€
        print("\n2ï¸âƒ£ ì—ì´ì „íŠ¸ í‰ê°€ ì¤‘...")
        avg_payoff, win_rate, eval_payoffs = evaluate_agent(agent, num_games=100)
        
        # ê²°ê³¼ ì‹œê°í™”
        print("\n3ï¸âƒ£ ê²°ê³¼ ì‹œê°í™”...")
        plot_training_results(rewards, losses, win_rates)
        
        print(f"\nğŸŠ í›ˆë ¨ ì™„ë£Œ!")
        print(f"ìµœì¢… ì„±ëŠ¥: ìŠ¹ë¥  {win_rate:.1%}, í‰ê·  ë³´ìƒ {avg_payoff:.3f}")
        
        return agent
        
    except Exception as e:
        print(f"âŒ ë©”ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    trained_agent = main()